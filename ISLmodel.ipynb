{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-08-15T11:13:44.850379Z","iopub.status.busy":"2022-08-15T11:13:44.849905Z","iopub.status.idle":"2022-08-15T11:13:45.231507Z","shell.execute_reply":"2022-08-15T11:13:45.229878Z","shell.execute_reply.started":"2022-08-15T11:13:44.850278Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import cv2\n","import os\n","from scipy import ndimage\n","from scipy.ndimage import convolve\n","from scipy import misc\n","import pandas as pd"]},{"cell_type":"markdown","metadata":{},"source":["**Preprocessing**"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-08-15T11:13:47.980417Z","iopub.status.busy":"2022-08-15T11:13:47.980032Z","iopub.status.idle":"2022-08-15T11:13:48.009233Z","shell.execute_reply":"2022-08-15T11:13:48.007988Z","shell.execute_reply.started":"2022-08-15T11:13:47.980385Z"},"trusted":true},"outputs":[],"source":["sigma=1.4\n","kernel_size=5\n","lowthreshold=0.09\n","highthreshold=0.17\n","strong_pixel=255\n","weak_pixel=100\n","img_smoothed = None\n","gradientMat = None\n","thetaMat = None\n","nonMaxImg = None\n","thresholdImg = None\n","\n","def rgb2gray(rgb):\n","    r, g, b = rgb[:,:,0], rgb[:,:,1], rgb[:,:,2]\n","    gray = 0.2989 * r + 0.5870 * g + 0.1140 * b\n","    return gray\n","\n","def gaussian_kernel(size, sigma=1):\n","    size = int(size) // 2\n","    x, y = np.mgrid[-size:size+1, -size:size+1]\n","    normal = 1 / (2.0 * np.pi * sigma**2)\n","    g =  np.exp(-((x**2 + y**2) / (2.0*sigma**2))) * normal\n","    return g\n","\n","def sobel_filters(img):\n","    Kx = np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], np.float32)\n","    Ky = np.array([[1, 2, 1], [0, 0, 0], [-1, -2, -1]], np.float32)\n","    Ix = ndimage.convolve(img, Kx)\n","    Iy = ndimage.convolve(img, Ky)\n","    G = np.hypot(Ix, Iy)\n","    G = G / G.max() * 255\n","    theta = np.arctan2(Iy, Ix)\n","    return (G, theta)\n","\n","def non_max_suppression(img, D):\n","    M, N = img.shape\n","    Z = np.zeros((M,N), dtype=np.int32)\n","    angle = D * 180. / np.pi\n","    angle[angle < 0] += 180\n","    for i in range(1,M-1):\n","        for j in range(1,N-1):\n","            try:\n","                q = 255\n","                r = 255\n","                if (0 <= angle[i,j] < 22.5) or (157.5 <= angle[i,j] <= 180):\n","                    q = img[i, j+1]\n","                    r = img[i, j-1]\n","                elif (22.5 <= angle[i,j] < 67.5):\n","                    q = img[i+1, j-1]\n","                    r = img[i-1, j+1]\n","                elif (67.5 <= angle[i,j] < 112.5):\n","                    q = img[i+1, j]\n","                    r = img[i-1, j]\n","                elif (112.5 <= angle[i,j] < 157.5):\n","                    q = img[i-1, j-1]\n","                    r = img[i+1, j+1]\n","                if (img[i,j] >= q) and (img[i,j] >= r):\n","                    Z[i,j] = img[i,j]\n","                else:\n","                    Z[i,j] = 0\n","            except IndexError as e:\n","                pass\n","    return Z\n","\n","def threshold(img):\n","    global highthreshold\n","    global lowthreshold\n","    global strong_pixel\n","    global weak_pixel\n","    highThresholdP = img.max() * highthreshold;\n","    lowThresholdP = highThresholdP * lowthreshold;\n","    M, N = img.shape\n","    res = np.zeros((M,N), dtype=np.int32)\n","    weak = np.int32(weak_pixel)\n","    strong = np.int32(strong_pixel)\n","    strong_i, strong_j = np.where(img >= highThresholdP)\n","    zeros_i, zeros_j = np.where(img < lowThresholdP)\n","    weak_i, weak_j = np.where((img <= highThresholdP) & (img >= lowThresholdP))\n","    res[strong_i, strong_j] = strong\n","    res[weak_i, weak_j] = weak\n","    return (res)\n","\n","def hysteresis(img):\n","    global strong_pixel\n","    global weak_pixel\n","    M, N = img.shape\n","    weak = weak_pixel\n","    strong = strong_pixel\n","    for i in range(1, M-1):\n","        for j in range(1, N-1):\n","            if (img[i,j] == weak):\n","                try:\n","                    if ((img[i+1, j-1] == strong) or (img[i+1, j] == strong) or (img[i+1, j+1] == strong)\n","                        or (img[i, j-1] == strong) or (img[i, j+1] == strong)\n","                        or (img[i-1, j-1] == strong) or (img[i-1, j] == strong) or (img[i-1, j+1] == strong)):\n","                        img[i, j] = strong\n","                    else:\n","                        img[i, j] = 0\n","                except IndexError as e:\n","                    pass\n","    return img\n","\n","def canny_preprocessing(img):\n","    img_smoothed = convolve(img, gaussian_kernel(kernel_size, sigma))\n","    gradientMat, thetaMat = sobel_filters(img_smoothed)\n","    nonMaxImg = non_max_suppression(gradientMat, thetaMat)\n","    thresholdImg = threshold(nonMaxImg)\n","    img_final = hysteresis(thresholdImg)\n","    return img_final\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-08-15T11:13:49.884727Z","iopub.status.busy":"2022-08-15T11:13:49.882301Z","iopub.status.idle":"2022-08-15T11:13:49.894523Z","shell.execute_reply":"2022-08-15T11:13:49.893378Z","shell.execute_reply.started":"2022-08-15T11:13:49.884680Z"},"trusted":true},"outputs":[],"source":["classified_image=[\"train\",\"test\"]\n","categories=[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\"]\n","category_index={\"0\":0,\"1\":1,\"2\":2,\"3\":3,\"4\":4,\"5\":5,\"6\":6,\"7\":7,\"8\":8,\"9\":9,\"A\":10,\"B\":11,\"C\":12,\"D\":13,\"E\":14,\"F\":15,\"G\":16,\"H\":17,\"I\":18,\"J\":19,\"K\":20,\"L\":21,\"M\":22,\"N\":23,\"O\":24,\"P\":25,\"Q\":26,\"R\":27,\"S\":28,\"T\":29,\"U\":30,\"V\":31,\"W\":32,\"X\":33,\"Y\":34,\"Z\":35}\n","\n","x_train=[]    #the values of trained images are stored in this array\n","y_train=[]    #the indexes of trained images that are beloged to their category are stored in this array\n","x_test=[]     #the values of tested images are stored in this array\n","y_test=[]     #the indexes of that tested images that are beloged to their category are stored in this array"]},{"cell_type":"markdown","metadata":{},"source":["Change the paths accordingly and run the below cell"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Extracting the train images/.......\n","    loading images of the category 0 in train images.......\n","    loading images of the category 1 in train images.......\n","    loading images of the category 2 in train images.......\n","    loading images of the category 3 in train images.......\n","    loading images of the category 4 in train images.......\n","    loading images of the category 5 in train images.......\n","    loading images of the category 6 in train images.......\n","    loading images of the category 7 in train images.......\n","    loading images of the category 8 in train images.......\n","    loading images of the category 9 in train images.......\n","    loading images of the category A in train images.......\n","    loading images of the category B in train images.......\n","    loading images of the category C in train images.......\n","    loading images of the category D in train images.......\n","    loading images of the category E in train images.......\n","    loading images of the category F in train images.......\n","    loading images of the category G in train images.......\n","    loading images of the category H in train images.......\n","    loading images of the category I in train images.......\n","    loading images of the category J in train images.......\n","    loading images of the category K in train images.......\n","    loading images of the category L in train images.......\n","    loading images of the category M in train images.......\n","    loading images of the category N in train images.......\n","    loading images of the category O in train images.......\n","    loading images of the category P in train images.......\n","    loading images of the category Q in train images.......\n","    loading images of the category R in train images.......\n","    loading images of the category S in train images.......\n","    loading images of the category T in train images.......\n","    loading images of the category U in train images.......\n","    loading images of the category V in train images.......\n","    loading images of the category W in train images.......\n","    loading images of the category X in train images.......\n","    loading images of the category Y in train images.......\n","    loading images of the category Z in train images.......\n","Extracting the test images/.......\n","    loading images of the category 0 in test images.......\n","    loading images of the category 1 in test images.......\n","    loading images of the category 2 in test images.......\n","    loading images of the category 3 in test images.......\n","    loading images of the category 4 in test images.......\n","    loading images of the category 5 in test images.......\n","    loading images of the category 6 in test images.......\n","    loading images of the category 7 in test images.......\n","    loading images of the category 8 in test images.......\n","    loading images of the category 9 in test images.......\n","    loading images of the category A in test images.......\n","    loading images of the category B in test images.......\n","    loading images of the category C in test images.......\n","    loading images of the category D in test images.......\n","    loading images of the category E in test images.......\n","    loading images of the category F in test images.......\n","    loading images of the category G in test images.......\n","    loading images of the category H in test images.......\n","    loading images of the category I in test images.......\n","    loading images of the category J in test images.......\n","    loading images of the category K in test images.......\n","    loading images of the category L in test images.......\n","    loading images of the category M in test images.......\n","    loading images of the category N in test images.......\n","    loading images of the category O in test images.......\n","    loading images of the category P in test images.......\n","    loading images of the category Q in test images.......\n","    loading images of the category R in test images.......\n","    loading images of the category S in test images.......\n","    loading images of the category T in test images.......\n","    loading images of the category U in test images.......\n","    loading images of the category V in test images.......\n","    loading images of the category W in test images.......\n","    loading images of the category X in test images.......\n","    loading images of the category Y in test images.......\n","    loading images of the category Z in test images.......\n","Fetching data completed........\n"]}],"source":["data_path=\"/Users/dhatrirukkannagari/Desktop/HackUTA/data1\"\n","\n","for classify_img in classified_image:\n","    \n","    path1=os.path.join(data_path,classify_img)   \n","    print(\"Extracting the \"+classify_img+\" images/.......\")\n","    \n","    for category in categories:\n","        folder_path=os.path.join(path1,category) \n","        images=os.listdir(folder_path)\n","        \n","        print(f'    loading images of the category {category} in {classify_img} images.......')\n","        \n","        for image in images:\n","            img_path=os.path.join(folder_path,image)\n","            try:\n","                img=cv2.imread(img_path)\n","                img=rgb2gray(img)\n","                preprocessed_img=canny_preprocessing(img)\n","                if classify_img==\"train\":\n","                    x_train.append(preprocessed_img)\n","                    y_train.append(category_index[category])\n","                else:\n","                    x_test.append(preprocessed_img)\n","                    y_test.append(category_index[category])\n","                    \n","            \n","            except:\n","                pass\n","\n","print(\"Fetching data completed........\")"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-08-15T11:14:16.783697Z","iopub.status.busy":"2022-08-15T11:14:16.783341Z","iopub.status.idle":"2022-08-15T11:14:16.789026Z","shell.execute_reply":"2022-08-15T11:14:16.787864Z","shell.execute_reply.started":"2022-08-15T11:14:16.783668Z"},"trusted":true},"outputs":[],"source":["x_train=np.array(x_train)\n","y_train=np.array(y_train)\n","x_test=np.array(x_test)\n","y_test=np.array(y_test)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":["(1801, 261, 310)"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["x_train.shape"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/plain":["(720, 261, 310)"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["x_test.shape"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["(1801,)"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["y_train.shape"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":["(720,)"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["y_test.shape"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-08-15T11:14:21.352540Z","iopub.status.busy":"2022-08-15T11:14:21.352166Z","iopub.status.idle":"2022-08-15T11:14:21.358489Z","shell.execute_reply":"2022-08-15T11:14:21.357653Z","shell.execute_reply.started":"2022-08-15T11:14:21.352512Z"},"trusted":true},"outputs":[],"source":["#Althogh when we converted to grayscale the imgs still have 3 color channels(BGR),so we have to RESHAPE it\n","x_train=np.reshape(x_train,(x_train.shape[0],261,310,1))\n","x_test=np.reshape(x_test,(x_test.shape[0],261,310,1))"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-08-15T11:14:24.360499Z","iopub.status.busy":"2022-08-15T11:14:24.359807Z","iopub.status.idle":"2022-08-15T11:14:24.364461Z","shell.execute_reply":"2022-08-15T11:14:24.363274Z","shell.execute_reply.started":"2022-08-15T11:14:24.360461Z"},"trusted":true},"outputs":[],"source":["#there are 36 categories so..\n","numOfCategories=36"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["from keras.utils import to_categorical\n","y_train=to_categorical(y_train,numOfCategories)\n","y_test=to_categorical(y_test,numOfCategories)"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["import tensorflow as tf"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["import keras"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["\n","from keras.models import Sequential\n","from keras.layers import Dense,Flatten,Dropout\n","from keras.layers import Conv2D,MaxPooling2D\n","from keras.layers import BatchNormalization"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["model=Sequential()\n","\n","model.add(Conv2D(64,kernel_size=(3,3),input_shape=(261, 310, 1),activation=\"relu\"))\n","model.add(BatchNormalization())\n","model.add(Conv2D(64,kernel_size=(3,3),input_shape=(261, 310, 1),activation=\"relu\"))\n","model.add(BatchNormalization())\n","model.add(MaxPooling2D((2,2)))\n","model.add(Dropout(0.4))\n","\n","model.add(Conv2D(128,kernel_size=(3,3),activation=\"relu\"))\n","model.add(BatchNormalization())\n","model.add(Conv2D(128,kernel_size=(3,3),activation=\"relu\"))\n","model.add(BatchNormalization())\n","model.add(MaxPooling2D((2,2)))\n","model.add(Dropout(0.4))\n","\n","model.add(Conv2D(256,kernel_size=(3,3),activation=\"relu\"))\n","model.add(BatchNormalization())\n","model.add(Conv2D(256,kernel_size=(3,3),activation=\"relu\"))\n","model.add(BatchNormalization())\n","model.add(MaxPooling2D((2,2)))\n","model.add(Dropout(0.4))\n","\n","model.add(Conv2D(512,kernel_size=(3,3),activation=\"relu\"))\n","model.add(BatchNormalization())\n","model.add(Conv2D(512,kernel_size=(3,3),activation=\"relu\"))\n","model.add(BatchNormalization())\n","model.add(MaxPooling2D((2,2)))\n","model.add(Dropout(0.4))\n","\n","\n","\n","model.add(Flatten())\n","model.add(Dropout(0.5))\n","model.add(Dense(128,activation=\"relu\"))\n","model.add(Dense(64,activation=\"relu\"))\n","model.add(Dense(36,activation=\"softmax\"))"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 259, 308, 64)      640       \n","                                                                 \n"," batch_normalization (Batch  (None, 259, 308, 64)      256       \n"," Normalization)                                                  \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 257, 306, 64)      36928     \n","                                                                 \n"," batch_normalization_1 (Bat  (None, 257, 306, 64)      256       \n"," chNormalization)                                                \n","                                                                 \n"," max_pooling2d (MaxPooling2  (None, 128, 153, 64)      0         \n"," D)                                                              \n","                                                                 \n"," dropout (Dropout)           (None, 128, 153, 64)      0         \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 126, 151, 128)     73856     \n","                                                                 \n"," batch_normalization_2 (Bat  (None, 126, 151, 128)     512       \n"," chNormalization)                                                \n","                                                                 \n"," conv2d_3 (Conv2D)           (None, 124, 149, 128)     147584    \n","                                                                 \n"," batch_normalization_3 (Bat  (None, 124, 149, 128)     512       \n"," chNormalization)                                                \n","                                                                 \n"," max_pooling2d_1 (MaxPoolin  (None, 62, 74, 128)       0         \n"," g2D)                                                            \n","                                                                 \n"," dropout_1 (Dropout)         (None, 62, 74, 128)       0         \n","                                                                 \n"," conv2d_4 (Conv2D)           (None, 60, 72, 256)       295168    \n","                                                                 \n"," batch_normalization_4 (Bat  (None, 60, 72, 256)       1024      \n"," chNormalization)                                                \n","                                                                 \n"," conv2d_5 (Conv2D)           (None, 58, 70, 256)       590080    \n","                                                                 \n"," batch_normalization_5 (Bat  (None, 58, 70, 256)       1024      \n"," chNormalization)                                                \n","                                                                 \n"," max_pooling2d_2 (MaxPoolin  (None, 29, 35, 256)       0         \n"," g2D)                                                            \n","                                                                 \n"," dropout_2 (Dropout)         (None, 29, 35, 256)       0         \n","                                                                 \n"," conv2d_6 (Conv2D)           (None, 27, 33, 512)       1180160   \n","                                                                 \n"," batch_normalization_6 (Bat  (None, 27, 33, 512)       2048      \n"," chNormalization)                                                \n","                                                                 \n"," conv2d_7 (Conv2D)           (None, 25, 31, 512)       2359808   \n","                                                                 \n"," batch_normalization_7 (Bat  (None, 25, 31, 512)       2048      \n"," chNormalization)                                                \n","                                                                 \n"," max_pooling2d_3 (MaxPoolin  (None, 12, 15, 512)       0         \n"," g2D)                                                            \n","                                                                 \n"," dropout_3 (Dropout)         (None, 12, 15, 512)       0         \n","                                                                 \n"," flatten (Flatten)           (None, 92160)             0         \n","                                                                 \n"," dropout_4 (Dropout)         (None, 92160)             0         \n","                                                                 \n"," dense (Dense)               (None, 128)               11796608  \n","                                                                 \n"," dense_1 (Dense)             (None, 64)                8256      \n","                                                                 \n"," dense_2 (Dense)             (None, 36)                2340      \n","                                                                 \n","=================================================================\n","Total params: 16499108 (62.94 MB)\n","Trainable params: 16495268 (62.92 MB)\n","Non-trainable params: 3840 (15.00 KB)\n","_________________________________________________________________\n"]}],"source":["model.summary()"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["model.compile(optimizer=\"adam\",loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","37/37 [==============================] - 511s 14s/step - loss: 4.3669 - accuracy: 0.3054 - val_loss: 175.6246 - val_accuracy: 0.0569\n","Epoch 2/10\n","37/37 [==============================] - 509s 14s/step - loss: 0.8405 - accuracy: 0.7551 - val_loss: 41.4877 - val_accuracy: 0.1083\n","Epoch 3/10\n","37/37 [==============================] - 515s 14s/step - loss: 0.2775 - accuracy: 0.9323 - val_loss: 14.0850 - val_accuracy: 0.1847\n","Epoch 4/10\n","37/37 [==============================] - 507s 14s/step - loss: 0.1411 - accuracy: 0.9634 - val_loss: 8.7234 - val_accuracy: 0.2722\n","Epoch 5/10\n","37/37 [==============================] - 1230s 34s/step - loss: 0.5118 - accuracy: 0.8717 - val_loss: 29.6007 - val_accuracy: 0.1847\n","Epoch 6/10\n","37/37 [==============================] - 497s 13s/step - loss: 0.2195 - accuracy: 0.9445 - val_loss: 16.3940 - val_accuracy: 0.2236\n","Epoch 7/10\n","37/37 [==============================] - 500s 14s/step - loss: 0.0415 - accuracy: 0.9872 - val_loss: 11.5884 - val_accuracy: 0.3056\n","Epoch 8/10\n","37/37 [==============================] - 501s 14s/step - loss: 0.0325 - accuracy: 0.9911 - val_loss: 7.5803 - val_accuracy: 0.4347\n","Epoch 9/10\n","37/37 [==============================] - 503s 14s/step - loss: 0.0169 - accuracy: 0.9950 - val_loss: 7.2539 - val_accuracy: 0.4431\n","Epoch 10/10\n","37/37 [==============================] - 515s 14s/step - loss: 0.0195 - accuracy: 0.9956 - val_loss: 6.5051 - val_accuracy: 0.4611\n"]},{"data":{"text/plain":["<keras.src.callbacks.History at 0x34d7dc550>"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["model.fit(x_train,y_train,batch_size=50,epochs=10,validation_data=(x_test,y_test))\n"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["#Saving the model\n","model_json=model.to_json()\n","with open(\"isl_model.json\",\"w\") as json_file:\n","    json_file.write(model_json)\n","    #serializing weights to HDF5\n","model.save_weights(\"isl_model.h5\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":4}
